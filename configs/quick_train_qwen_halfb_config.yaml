defaults:
  - ppo_trainer
  - agents
  - base
  - _self_

system:
  CUDA_VISIBLE_DEVICES: 0

# ------ Rollout Configuration ------
rollout:
  agent_group_num: [8]
  agent_group_size: [16] # number of agents = agent_group_num * agent_group_size
  validation_agent_group_num: [64, 64]
  validation_agent_group_size: [1, 1]
  training: [simpleSokobanAgent]
  validation: [simpleSokobanAgent, largeSokobanAgent]
  validation_seed: 123
  truncation: left  # truncate from left (oldest tokens) to keep recent context
  use_turn_scores: False  # for GAE computation
  rollout_filter_ratio: 0.25  # filter ratio for rollout selection
  rollout_filter_type: std  # std or std_rev for filtering criteria
  reward_normalization:
    grouping: "state" # state / batch / inductive
    method: "identity" # asym_clip / identity / mean_std

# ------ Training Parameters ------
train_batch_size: 128
micro_batch_size_per_gpu: 4
ppo_micro_batch_size_per_gpu: 4
ppo_mini_batch_size: 32
total_training_steps: 200
n_gpus_per_node: 1
max_prompt_length: 4096
max_response_length: 400
test_freq: 10
save_freq: -1
gpu_memory_utilization: 0.5
model_path: Qwen/Qwen2.5-0.5B-Instruct
project_name: lmgame_train
experiment_name: qwen_half_b_think_sokoban
resume_mode: disable