INFO 05-18 03:23:24 [__init__.py:239] Automatically detected platform cuda.
/mbz/yichao/miniconda3/envs/lmgame/lib/python3.12/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In '_2_sokoban': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
config: {'data': {'tokenizer': None, 'train_files': '~/data/rlhf/gsm8k/train.parquet', 'val_files': '~/data/rlhf/gsm8k/test.parquet', 'prompt_key': 'prompt', 'max_prompt_length': None, 'max_response_length': None, 'train_batch_size': 128, 'val_batch_size': None, 'return_raw_input_ids': False, 'return_raw_chat': False, 'shuffle': True, 'filter_overlong_prompts': False, 'truncation': 'error', 'image_key': 'images'}, 'actor_rollout_ref': {'hybrid_engine': True, 'model': {'path': '${model_path}', 'external_lib': None, 'override_config': {}, 'enable_gradient_checkpointing': True, 'use_remove_padding': False}, 'actor': {'strategy': 'fsdp', 'ppo_mini_batch_size': 32, 'ppo_micro_batch_size': None, 'ppo_micro_batch_size_per_gpu': 4, 'use_dynamic_bsz': False, 'ppo_max_token_len_per_gpu': 16384, 'grad_clip': 1.0, 'clip_ratio': 0.2, 'entropy_coeff': 0.001, 'use_kl_loss': False, 'use_torch_compile': True, 'kl_loss_coef': 0.001, 'kl_loss_type': 'kl', 'ppo_epochs': 1, 'shuffle': False, 'ulysses_sequence_parallel_size': 1, 'checkpoint': {'contents': ['model', 'hf_model', 'optimizer', 'extra']}, 'optim': {'lr': 1e-06, 'lr_warmup_steps': -1, 'lr_warmup_steps_ratio': 0.0, 'min_lr_ratio': None, 'warmup_style': 'constant', 'total_training_steps': -1, 'betas': [0.9, 0.999]}, 'fsdp_config': {'wrap_policy': {'min_num_params': 0}, 'param_offload': False, 'optimizer_offload': False, 'fsdp_size': -1}, 'micro_batch_size_per_gpu': 4, 'use_ref': True, 'clip_ratio_low': 0.2, 'clip_ratio_high': 0.28}, 'ref': {'fsdp_config': {'param_offload': False, 'wrap_policy': {'min_num_params': 0}}, 'log_prob_micro_batch_size': None, 'log_prob_micro_batch_size_per_gpu': 4, 'log_prob_use_dynamic_bsz': '${actor_rollout_ref.actor.use_dynamic_bsz}', 'log_prob_max_token_len_per_gpu': '${actor_rollout_ref.actor.ppo_max_token_len_per_gpu}', 'ulysses_sequence_parallel_size': '${actor_rollout_ref.actor.ulysses_sequence_parallel_size}'}, 'rollout': {'name': 'vllm', 'temperature': 1, 'top_k': -1, 'top_p': 1, 'use_fire_sampling': False, 'prompt_length': 1, 'response_length': 400, 'dtype': 'bfloat16', 'gpu_memory_utilization': 0.5, 'ignore_eos': False, 'enforce_eager': False, 'free_cache_engine': False, 'load_format': 'dummy_dtensor', 'tensor_model_parallel_size': 1, 'max_num_batched_tokens': 20000, 'max_model_len': 20000, 'max_num_seqs': 1024, 'log_prob_micro_batch_size': None, 'log_prob_micro_batch_size_per_gpu': 4, 'log_prob_use_dynamic_bsz': '${actor_rollout_ref.actor.use_dynamic_bsz}', 'log_prob_max_token_len_per_gpu': '${actor_rollout_ref.actor.ppo_max_token_len_per_gpu}', 'disable_log_stats': True, 'enable_chunked_prefill': True, 'do_sample': True, 'n': 1, 'val_kwargs': {'top_k': -1, 'top_p': 1.0, 'temperature': 0, 'n': 1, 'do_sample': False}, 'rollout_filter_ratio': 0.25, 'rollout_filter_type': 'std'}}, 'critic': {'strategy': 'fsdp', 'optim': {'lr': 1e-05, 'lr_warmup_steps_ratio': 0.0, 'min_lr_ratio': None, 'warmup_style': 'constant', 'total_training_steps': -1, 'betas': [0.9, 0.999]}, 'model': {'path': '${model_path}', 'tokenizer_path': '${actor_rollout_ref.model.path}', 'override_config': {}, 'external_lib': '${actor_rollout_ref.model.external_lib}', 'enable_gradient_checkpointing': True, 'use_remove_padding': False, 'fsdp_config': {'param_offload': False, 'optimizer_offload': False, 'wrap_policy': {'min_num_params': 0}, 'fsdp_size': -1}}, 'ppo_mini_batch_size': 32, 'ppo_micro_batch_size': None, 'ppo_micro_batch_size_per_gpu': 4, 'forward_micro_batch_size': '${critic.ppo_micro_batch_size}', 'forward_micro_batch_size_per_gpu': '${critic.ppo_micro_batch_size_per_gpu}', 'use_dynamic_bsz': '${actor_rollout_ref.actor.use_dynamic_bsz}', 'ppo_max_token_len_per_gpu': 32768, 'forward_max_token_len_per_gpu': '${critic.ppo_max_token_len_per_gpu}', 'ulysses_sequence_parallel_size': 1, 'ppo_epochs': '${actor_rollout_ref.actor.ppo_epochs}', 'shuffle': '${actor_rollout_ref.actor.shuffle}', 'grad_clip': 1.0, 'cliprange_value': 0.5, 'checkpoint': {'contents': ['model', 'hf_model', 'optimizer', 'extra']}}, 'reward_model': {'enable': False, 'strategy': 'fsdp', 'model': {'input_tokenizer': '${actor_rollout_ref.model.path}', 'path': 'sfairXC/FsfairX-LLaMA3-RM-v0.1', 'external_lib': '${actor_rollout_ref.model.external_lib}', 'use_remove_padding': False, 'fsdp_config': {'wrap_policy': {'min_num_params': 0}, 'param_offload': False, 'fsdp_size': -1}}, 'micro_batch_size': None, 'micro_batch_size_per_gpu': None, 'max_length': None, 'ulysses_sequence_parallel_size': 1, 'use_dynamic_bsz': '${critic.use_dynamic_bsz}', 'forward_max_token_len_per_gpu': '${critic.forward_max_token_len_per_gpu}', 'reward_manager': 'naive'}, 'custom_reward_function': {'path': None, 'name': 'compute_score'}, 'algorithm': {'gamma': 1.0, 'lam': 1.0, 'adv_estimator': 'gae', 'kl_penalty': 'kl', 'kl_ctrl': {'type': 'fixed', 'kl_coef': 0.001}}, 'trainer': {'balance_batch': True, 'total_epochs': 30, 'total_training_steps': 200, 'project_name': 'train_with_think', 'experiment_name': 'sokoban', 'logger': ['console', 'wandb'], 'val_generations_to_log_to_wandb': 0, 'nnodes': 1, 'n_gpus_per_node': 1, 'save_freq': 100, 'resume_mode': 'disable', 'resume_from_path': None, 'test_freq': 10, 'critic_warmup': 0, 'default_hdfs_dir': None, 'del_local_ckpt_after_load': False, 'default_local_dir': 'checkpoints/${trainer.project_name}/${trainer.experiment_name}', 'max_actor_ckpt_to_keep': 'None', 'max_critic_ckpt_to_keep': 'None', 'validation_steps': 1, 'val_before_train': True, 'generations_to_log_to_wandb': {'train': 128, 'val': 20}, 'val_only': False}, 'custom_envs': {'SimpleSokoban': {'env_type': 'sokoban', 'max_actions_per_traj': 10, 'env_instruction': 'You are solving the Sokoban puzzle. You are the player and you need to push all boxes to targets. When you are right next to a box, you can push it by moving in the same direction. You cannot push a box through a wall, and you cannot pull a box. The answer should be a sequence of actions, like <answer>Right || Right || Up</answer>.', 'max_tokens': 100, 'max_turn': 5, 'env_config': {'dim_x': 6, 'dim_y': 6, 'num_boxes': 1, 'max_steps': 100}}, 'GSM8K_NoThink': {'env_type': 'gsm8k', 'max_actions_per_traj': 1, 'env_instruction': "You are solving Math problems. Let's think step by step. Always put the answer in integer at the end of your response. ", 'max_tokens': 100, 'max_turn': 1, 'env_config': {'max_steps': 5, 'split': 'train'}}, 'GSM8K_NoThink_Turn_5': {'env_type': 'gsm8k', 'max_actions_per_traj': 5, 'env_instruction': "You are solving Math problems. Let's think step by step. Always put the answer in integer at the end of your response. ", 'max_tokens': 100, 'max_turn': 5, 'env_config': {'max_steps': 5, 'split': 'train'}}, 'GSM8K': {'env_type': 'gsm8k', 'max_actions_per_traj': 1, 'env_instruction': "You are solving Math problems. Let's think step by step. You should put all thinking process into <think>...</think> and then only give the final answer in <answer>...</answer>.", 'max_tokens': 100, 'max_turn': 1, 'env_config': {'max_steps': 5, 'split': 'train'}}, 'GSM8K_Turn_5': {'env_type': 'gsm8k', 'max_actions_per_traj': 5, 'env_instruction': "You are solving Math problems. Let's think step by step. You should put all thinking process into <think>...</think> and then only give the final answer in <answer>...</answer>.", 'max_tokens': 100, 'max_turn': 5, 'env_config': {'max_steps': 5, 'split': 'train'}}, 'Blocksworld3_Sparse': {'env_type': 'blocksworld', 'max_actions_per_traj': 20, 'max_turn': 10, 'env_instruction': 'You are solving Blocksworld problems.\xa0Each state is represented by a graphical stack of blocks. There are 3 blocks: block 1, block 2, and block 3.\xa0You can only move a block if there is no other block on top of it.\xa0Each action should be formatted as:"move X to Y", where X is the block number you\'re moving, and Y is either\xa00, meaning move the block to the table; or\xa0another block number, meaning move it on top of that block.\n\nA state is like this:\n _ \n|2|\n ¯ \n _  _ \n|1||3|\n ¯  ¯ \nThis means: Block 2 is on top of block 1, and Block 1, 3 is on the table. Another example:\n _ \n|1|\n ¯ \n _ \n|3|\n ¯ \n _ \n|2|\n ¯ \nThis means: Block 1 is on top of block 3, Block 3 is on top of block 2, and Block 2 is on the table. \nYou are given the current state and the goal state, and you need to find a sequence of actions to move the blocks to the goal state. The answer should be a sequence of actions, like <answer>(move 2 to 0) || (move 1 to 2)</answer>', 'max_tokens': 100, 'env_config': {'num_blocks': 3, 'max_steps': 100, 'render_mode': '2d_sparse'}}, 'Blocksworld3_1D': {'env_type': 'blocksworld', 'max_actions_per_traj': 20, 'max_turn': 10, 'env_instruction': 'You are solving blocksworld problems. There are 3 blocks on the table. The state is represented by an array of 3 numbers, each number shows what the block is on. A value of 0 means the block is on the table. A value of 1, 2 or 3 means the block is on top of that block.\xa0For example, if the current state is [2, 0, 1], then\xa0block 1 is on top of Block 2, Block 2 is on the table, and Block 3 is on top of Block 1.\xa0Each action should be formatted as:"move X to Y", where X is the block number you\'re moving, and Y is either\xa00, meaning move the block to the table; or\xa0another block number, meaning move it on top of that block. You can only move a block if there is no other block on top of it.\xa0You are given the current state and the goal state, and you need to find a sequence of actions to move the blocks to the goal state. The answer should be a sequence of actions, like <answer>(move 2 to 0) || (move 1 to 2)</answer>', 'max_tokens': 100, 'env_config': {'num_blocks': 3, 'max_steps': 100, 'render_mode': '1d'}}, 'Blocksworld3_Text': {'env_type': 'blocksworld', 'max_actions_per_traj': 20, 'max_turn': 10, 'env_instruction': 'You are solving blocksworld problems. There are 3 blocks: block 1, block 2, and block 3.\xa0You can only move a block if there is no other block on top of it.\xa0Each action should be formatted as:"move X to Y", where X is the block number you\'re moving, and Y is either\xa00, meaning move the block to the table; or\xa0another block number, meaning move it on top of that block. You are given the current state and the goal state, and you need to find a sequence of actions to move the blocks to the goal state. The answer should be a sequence of actions, like <answer>(move 2 to 0) || (move 1 to 2)</answer>', 'max_tokens': 100, 'env_config': {'num_blocks': 3, 'max_steps': 100, 'render_mode': 'text'}}, 'Bird': {'env_type': 'bird', 'max_actions_per_traj': 1, 'max_turn': 1, 'env_instruction': 'Translate the question into SQL given database information. Replay with one fenced block that contains only SQL code:\n```sql\n...\n``\n.', 'max_tokens': 1024, 'env_config': {'max_steps': 5}}, 'Tetris_1': {'env_type': 'tetris', 'max_actions_per_traj': 40, 'max_turn': 10, 'env_instruction': 'You are solving a simplified version of Tetris puzzle. The game board is a 4*4 grid, and blocks fall one at a time from the top. There is only one type of block, that is 1×1 square marked in X. Your goal is to fill the last row completely with blocks to get points, with the state such as \n____\n____\n____\n____\nYou can only move the current block left, right, or down. The answer should be a sequence of actions, like <answer>Left || Right || Down</answer> Hint: If we see empty space (_) in the last row, try to left/right move the current block to let it drop down to the empty space.', 'max_tokens': 100, 'env_config': {'dim_x': 4, 'dim_y': 4, 'max_steps': 100, 'box_type': 1}}, 'Tetris_2': {'env_type': 'tetris', 'max_actions_per_traj': 20, 'max_turn': 10, 'env_instruction': 'You are solving a simplified version of Tetris puzzle. The game board is a 4*4 grid, and blocks fall one at a time from the top. Your goal is to fill the last row completely with blocks to get points, with the state such as \n____\n____\n____\n####\n. You can only move the current block left, right, or down. The answer should be a sequence of actions, like <answer>Left || Right || Down</answer> Hint: If we see empty space (_) in the last row, try to left/right move the current block to let it drop down to the empty space.', 'max_tokens': 100, 'env_config': {'dim_x': 4, 'dim_y': 4, 'max_steps': 100, 'box_type': 2}}, 'LargerSokoban_Dim_8': {'env_type': 'sokoban', 'max_actions_per_traj': 10, 'env_instruction': 'You are solving the Sokoban puzzle. You are the player and you need to push all boxes to targets. When you are right next to a box, you can push it by moving in the same direction. You cannot push a box through a wall, and you cannot pull a box. The answer should be a sequence of actions, like <answer>Right || Right || Up</answer>', 'max_tokens': 100, 'max_turn': 5, 'env_config': {'dim_x': 8, 'dim_y': 8, 'num_boxes': 1, 'max_steps': 100, 'search_depth': 30}}, 'WebShop': {'env_type': 'webshop', 'max_actions_per_traj': 30, 'max_turn': 10, 'env_instruction': 'You are browsing an online shop. Based on the instruction, find the product that matches the production description. You need to iteratively take action, read the website and decide what action to take next until finding the product. Available actions depends on the page: in the search page you can search keywords, in the search result page you can click an item url or click[next >] to navigate to next page, in the product page you can click[description] or click[features] to see the details, click[blue] or click[x-large] to choose size and colors, click[buy now] when you decided to buy the product, click[back to search] to return to search page. Example process: I need a gingko light and 20x20 pillow cover that is hand painted. First search[gingko light 20x20 pillow cover hand painted], answer format: <answer>search[blanket with fleece throw]</answer>. Valid answer is search[<keywords>] or click[<clickable>].', 'max_tokens': 100, 'env_config': None}}, 'system': {'CUDA_VISIBLE_DEVICES': 1}, 'micro_batch_size_per_gpu': 4, 'ppo_mini_batch_size': 32, 'model_path': 'Qwen/Qwen2.5-0.5B-Instruct', 'agent_proxy': {'max_turn': 20, 'action_sep': '||', 'max_actions_per_turn': 5, 'use_turn_scores': False, 'enable_think': True, 'reward_normalization': {'grouping': 'state', 'method': 'identity'}}, 'es_manager': {'format_penalty': -0.1, 'train': {'env_groups': 8, 'group_size': 16, 'env_configs': {'tags': ['SimpleSokoban'], 'n_groups': [8]}}, 'val': {'env_groups': 256, 'group_size': 1, 'env_configs': {'tags': ['SimpleSokoban'], 'n_groups': [256]}}}, 'ctx_manager': {'generation': {'gen_config': {'response_length': '${actor_rollout_ref.rollout.response_length}', 'temperature': '${actor_rollout_ref.rollout.temperature}', 'top_p': '${actor_rollout_ref.rollout.top_p}', 'top_k': '${actor_rollout_ref.rollout.top_k}', 'kwargs': None}}}}2025-05-18 03:23:39,187	INFO worker.py:1888 -- Started a local Ray instance.
[36m(TaskRunner pid=1436997)[0m DeprecationWarning: `ray.state.available_resources_per_node` is a private attribute and access will be removed in a future Ray version.
[36m(WorkerDict pid=1438062)[0m Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForTokenClassification is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
[36m(WorkerDict pid=1438062)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[36m(WorkerDict pid=1438062)[0m Some weights of Qwen2ForTokenClassification were not initialized from the model checkpoint at Qwen/Qwen2.5-0.5B-Instruct and are newly initialized: ['score.bias', 'score.weight']
[36m(WorkerDict pid=1438062)[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[36m(WorkerDict pid=1438062)[0m /mbz/yichao/miniconda3/envs/lmgame/lib/python3.12/site-packages/torch/distributed/fsdp/_init_utils.py:444: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.FULL_SHARD since the world size is 1.
[36m(WorkerDict pid=1438062)[0m   warnings.warn(
[36m(WorkerDict pid=1438062)[0m /mbz/yichao/miniconda3/envs/lmgame/lib/python3.12/site-packages/torch/distributed/fsdp/_init_utils.py:444: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.FULL_SHARD since the world size is 1.
[36m(WorkerDict pid=1438062)[0m   warnings.warn(
[36m(WorkerDict pid=1438062)[0m Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
[36m(WorkerDict pid=1438062)[0m /mbz/yichao/miniconda3/envs/lmgame/lib/python3.12/site-packages/torch/distributed/fsdp/_init_utils.py:444: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.FULL_SHARD since the world size is 1.
[36m(WorkerDict pid=1438062)[0m   warnings.warn(
[36m(WorkerDict pid=1438062)[0m Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
[36m(WorkerDict pid=1438062)[0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.71it/s]
[36m(WorkerDict pid=1438062)[0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.70it/s]
[36m(WorkerDict pid=1438062)[0m 
[36m(WorkerDict pid=1438062)[0m /mbz/yichao/miniconda3/envs/lmgame/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
[36m(WorkerDict pid=1438062)[0m   warnings.warn(
[36m(TaskRunner pid=1436997)[0m wandb: Currently logged in as: mhuo (gamebench) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[36m(TaskRunner pid=1436997)[0m wandb: creating run
[36m(TaskRunner pid=1436997)[0m wandb: Tracking run with wandb version 0.19.11
[36m(TaskRunner pid=1436997)[0m wandb: Run data is saved locally in /mbz/mingjia/lmgame-bench/wandb/run-20250518_032853-7otedn2l
[36m(TaskRunner pid=1436997)[0m wandb: Run `wandb offline` to turn off syncing.
[36m(TaskRunner pid=1436997)[0m wandb: Syncing run sokoban
[36m(TaskRunner pid=1436997)[0m wandb: ⭐️ View project at https://wandb.ai/gamebench/train_with_think
[36m(TaskRunner pid=1436997)[0m wandb: 🚀 View run at https://wandb.ai/gamebench/train_with_think/runs/7otedn2l
[36m(TaskRunner pid=1436997)[0m Training Progress:   0%|          | 0/200 [00:00<?, ?it/s]

CUDA_VISIBLE_DEVICES: 1
[36m(TaskRunner pid=1436997)[0m [DEBUG] using ref policy
[36m(TaskRunner pid=1436997)[0m using dummy reward manager
[36m(TaskRunner pid=1436997)[0m [validate_config] All configuration checks passed successfully!
[36m(TaskRunner pid=1436997)[0m Total training steps: 200
[36m(WorkerDict pid=1438062)[0m Critic overriding config {'bos_token_id': None, 'eos_token_id': 151645, 'pad_token_id': 151643}
[36m(WorkerDict pid=1438062)[0m Qwen2ForTokenClassification contains 494.03M parameters
[36m(WorkerDict pid=1438062)[0m Before critic FSDP, memory allocated (GB): 0.0, memory reserved (GB): 0.0
[36m(WorkerDict pid=1438062)[0m NCCL version 2.21.5+cuda12.4
[36m(WorkerDict pid=1438062)[0m After critic FSDP, memory allocated (GB): 1.8410840034484863, memory reserved (GB): 2.44921875
[36m(WorkerDict pid=1438062)[0m Total steps: 200, num_warmup_steps: 0
[36m(WorkerDict pid=1438062)[0m Critic use_remove_padding=False
[36m(WorkerDict pid=1438062)[0m Model config after override: Qwen2Config {
[36m(WorkerDict pid=1438062)[0m   "architectures": [
[36m(WorkerDict pid=1438062)[0m     "Qwen2ForCausalLM"
[36m(WorkerDict pid=1438062)[0m   ],
[36m(WorkerDict pid=1438062)[0m   "attention_dropout": 0.0,
[36m(WorkerDict pid=1438062)[0m   "eos_token_id": 151645,
[36m(WorkerDict pid=1438062)[0m   "hidden_act": "silu",
[36m(WorkerDict pid=1438062)[0m   "hidden_size": 896,
[36m(WorkerDict pid=1438062)[0m   "initializer_range": 0.02,
[36m(WorkerDict pid=1438062)[0m   "intermediate_size": 4864,
[36m(WorkerDict pid=1438062)[0m   "max_position_embeddings": 32768,
[36m(WorkerDict pid=1438062)[0m   "max_window_layers": 21,
[36m(WorkerDict pid=1438062)[0m   "model_type": "qwen2",
[36m(WorkerDict pid=1438062)[0m   "num_attention_heads": 14,
[36m(WorkerDict pid=1438062)[0m   "num_hidden_layers": 24,
[36m(WorkerDict pid=1438062)[0m   "num_key_value_heads": 2,
[36m(WorkerDict pid=1438062)[0m   "pad_token_id": 151643,
[36m(WorkerDict pid=1438062)[0m   "rms_norm_eps": 1e-06,
[36m(WorkerDict pid=1438062)[0m   "rope_scaling": null,
[36m(WorkerDict pid=1438062)[0m   "rope_theta": 1000000.0,
[36m(WorkerDict pid=1438062)[0m   "sliding_window": 32768,
[36m(WorkerDict pid=1438062)[0m   "tie_word_embeddings": true,
[36m(WorkerDict pid=1438062)[0m   "torch_dtype": "bfloat16",
[36m(WorkerDict pid=1438062)[0m   "transformers_version": "4.51.3",
[36m(WorkerDict pid=1438062)[0m   "use_cache": true,
[36m(WorkerDict pid=1438062)[0m   "use_sliding_window": false,
[36m(WorkerDict pid=1438062)[0m   "vocab_size": 151936
[36m(WorkerDict pid=1438062)[0m }
[36m(WorkerDict pid=1438062)[0m 
[36m(WorkerDict pid=1438062)[0m Qwen2ForCausalLM contains 494.03M parameters
[36m(WorkerDict pid=1438062)[0m wrap_policy: functools.partial(<function _or_policy at 0x14c7f4531b20>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x14c7f45319e0>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])
[36m(WorkerDict pid=1438062)[0m Actor use_remove_padding=False
[36m(WorkerDict pid=1438062)[0m Model config after override: Qwen2Config {
[36m(WorkerDict pid=1438062)[0m   "architectures": [
[36m(WorkerDict pid=1438062)[0m     "Qwen2ForCausalLM"
[36m(WorkerDict pid=1438062)[0m   ],
[36m(WorkerDict pid=1438062)[0m   "attention_dropout": 0.0,
[36m(WorkerDict pid=1438062)[0m   "eos_token_id": 151645,
[36m(WorkerDict pid=1438062)[0m   "hidden_act": "silu",
[36m(WorkerDict pid=1438062)[0m   "hidden_size": 896,
[36m(WorkerDict pid=1438062)[0m   "initializer_range": 0.02,
[36m(WorkerDict pid=1438062)[0m   "intermediate_size": 4864,
[36m(WorkerDict pid=1438062)[0m   "max_position_embeddings": 32768,
[36m(WorkerDict pid=1438062)[0m   "max_window_layers": 21,
[36m(WorkerDict pid=1438062)[0m   "model_type": "qwen2",
[36m(WorkerDict pid=1438062)[0m   "num_attention_heads": 14,
[36m(WorkerDict pid=1438062)[0m   "num_hidden_layers": 24,
[36m(WorkerDict pid=1438062)[0m   "num_key_value_heads": 2,
[36m(WorkerDict pid=1438062)[0m   "pad_token_id": 151643,
[36m(WorkerDict pid=1438062)[0m   "rms_norm_eps": 1e-06,
[36m(WorkerDict pid=1438062)[0m   "rope_scaling": null,
[36m(WorkerDict pid=1438062)[0m   "rope_theta": 1000000.0,
[36m(WorkerDict pid=1438062)[0m   "sliding_window": 32768,
[36m(WorkerDict pid=1438062)[0m   "tie_word_embeddings": true,
[36m(WorkerDict pid=1438062)[0m   "torch_dtype": "bfloat16",
[36m(WorkerDict pid=1438062)[0m   "transformers_version": "4.51.3",
[36m(WorkerDict pid=1438062)[0m   "use_cache": true,
[36m(WorkerDict pid=1438062)[0m   "use_sliding_window": false,
[36m(WorkerDict pid=1438062)[0m   "vocab_size": 151936
[36m(WorkerDict pid=1438062)[0m }
[36m(WorkerDict pid=1438062)[0m 
[36m(WorkerDict pid=1438062)[0m Qwen2ForCausalLM contains 494.03M parameters
[36m(WorkerDict pid=1438062)[0m wrap_policy: functools.partial(<function _or_policy at 0x14c7f4531b20>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x14c7f45319e0>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])
[36m(WorkerDict pid=1438062)[0m Total steps: 200, num_warmup_steps: 0
[36m(WorkerDict pid=1438062)[0m Actor use_remove_padding=False
[36m(WorkerDict pid=1438062)[0m Before building vllm rollout, memory allocated (GB): 4.602716445922852, memory reserved (GB): 5.28125
[36m(WorkerDict pid=1438062)[0m WARNING 05-18 03:27:55 [utils.py:2321] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x14c4c8295220>
[36m(WorkerDict pid=1438062)[0m WARNING 05-18 03:27:57 [topk_topp_sampler.py:63] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[36m(WorkerDict pid=1438062)[0m kwargs: {'n': 1, 'logprobs': 0, 'max_tokens': 400, 'detokenize': False, 'temperature': 1, 'top_k': -1, 'top_p': 1, 'ignore_eos': False}
[36m(WorkerDict pid=1438062)[0m After building vllm rollout, memory allocated (GB): 33.17864418029785, memory reserved (GB): 38.873046875
[36m(WorkerDict pid=1438062)[0m After building sharding manager, memory allocated (GB): 33.17864418029785, memory reserved (GB): 38.873046875
[36m(TaskRunner pid=1436997)[0m Using LocalLogger is deprecated. The constructor API will change 
[36m(TaskRunner pid=1436997)[0m test_gen_batch meta info: {'eos_token_id': 151645, 'pad_token_id': 151643, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
[36m(TaskRunner pid=1436997)[0m validation generation time: 103.5480215549469 seconds
[36m(TaskRunner pid=1436997)[0m ("Initial validation metrics: {'val/SimpleSokoban/success': 0.12109375, "
[36m(TaskRunner pid=1436997)[0m  "'val/SimpleSokoban/num_actions': 7.671875, "
[36m(TaskRunner pid=1436997)[0m  "'val/SimpleSokoban/action_is_effective': 0.5501302083333334, "
[36m(TaskRunner pid=1436997)[0m  "'val/SimpleSokoban/action_is_valid': 0.8268229166666667, "
[36m(TaskRunner pid=1436997)[0m  "'val/response_length': 339.71484375, 'val/test_score/unknown': "
[36m(TaskRunner pid=1436997)[0m  '0.46093749161809683}')
[36m(TaskRunner pid=1436997)[0m step:0 - val/SimpleSokoban/success:0.121 - val/SimpleSokoban/num_actions:7.672 - val/SimpleSokoban/action_is_effective:0.550 - val/SimpleSokoban/action_is_valid:0.827 - val/response_length:339.715 - val/test_score/unknown:0.461
[36m(TaskRunner pid=1436997)[0m step:1 - train/SimpleSokoban/success:0.008 - train/SimpleSokoban/num_actions:3.039 - train/SimpleSokoban/action_is_effective:0.127 - train/SimpleSokoban/action_is_valid:0.290 - train/response_length:460.797 - global_seqlen/min:35619.000 - global_seqlen/max:35619.000 - global_seqlen/minmax_diff:0.000 - global_seqlen/balanced_min:35619.000 - global_seqlen/balanced_max:35619.000 - global_seqlen/mean:35619.000 - critic/kl:0.005 - critic/kl_coeff:0.001 - critic/vf_loss:6.644 - critic/vf_clipfrac:0.000 - critic/vpred_mean:0.710 - critic/grad_norm:493.519 - perf/mfu/critic:0.086 - critic/lr:0.000 - actor/entropy_loss:1.160 - actor/pg_loss:-0.008 - actor/pg_clipfrac:0.000 - actor/ppo_kl:0.000 - actor/grad_norm:26.405 - perf/mfu/actor:0.054 - perf/max_memory_allocated_gb:83.593 - perf/max_memory_reserved_gb:91.367 - perf/cpu_memory_used_gb:20.782 - actor/lr:0.000 - critic/score/mean:-0.491 - critic/score/max:10.200 - critic/score/min:-1.300 - critic/rewards/mean:-0.495 - critic/rewards/max:10.198 - critic/rewards/min:-1.305 - critic/advantages/mean:-0.001 - critic/advantages/max:5.742 - critic/advantages/min:-3.390 - critic/returns/mean:-0.614 - critic/returns/max:10.201 - critic/returns/min:-1.306 - critic/values/mean:0.715 - critic/values/max:12.062 - critic/values/min:-16.250 - critic/vf_explained_var:-4.955 - response_length/mean:1113.094 - response_length/max:2390.000 - response_length/min:549.000 - response_length/clip_ratio:0.000 - prompt_length/mean:0.000 - prompt_length/max:0.000 - prompt_length/min:0.000 - prompt_length/clip_ratio:0.000 - timing_s/gen:29.921 - timing_s/old_log_prob:0.972 - timing_s/ref:2.340 - timing_s/values:0.442 - timing_s/adv:0.082 - timing_s/update_critic:1.745 - timing_s/update_actor:2.797 - timing_s/step:38.301 - timing_per_token_ms/update_critic:0.049 - timing_per_token_ms/update_actor:0.079 - timing_per_token_ms/gen:0.840 - timing_per_token_ms/ref:0.066 - timing_per_token_ms/adv:0.002 - timing_per_token_ms/values:0.012 - perf/total_num_tokens:35619.000 - perf/time_per_step:38.301 - perf/throughput:929.966 - timing_s/total:38.304[36m(TaskRunner pid=1436997)[0m Training Progress:   0%|          | 1/200 [00:38<2:07:02, 38.31s/it]
[36m(TaskRunner pid=1436997)[0m Training Progress:   1%|          | 2/200 [01:11<1:57:05, 35.48s/it]
[36m(TaskRunner pid=1436997)[0m Training Progress:   2%|▏         | 3/200 [01:51<2:02:20, 37.26s/it]
[36m(TaskRunner pid=1436997)[0m Training Progress:   2%|▏         | 4/200 [02:27<2:00:19, 36.83s/it]
